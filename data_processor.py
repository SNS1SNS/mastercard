"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∏–µ–Ω—Ç–æ–≤
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, Any
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import RobustScaler
import warnings
warnings.filterwarnings('ignore')

from config import config

class DataProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    def __init__(self, data_path: str):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
        self.df = pd.read_parquet(data_path)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.df.shape[0]:,} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
        
        self.client_features = None
        self.processed_features = None
        self.feature_descriptions = self._get_feature_descriptions()
        
    @property
    def data(self):
        """–°–≤–æ–π—Å—Ç–≤–æ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∞–Ω–Ω—ã–º (–∞–ª–∏–∞—Å –¥–ª—è df)"""
        return self.df
    
    @data.setter
    def data(self, value):
        """–°–µ—Ç—Ç–µ—Ä –¥–ª—è –¥–∞–Ω–Ω—ã—Ö"""
        self.df = value
        
    def _get_feature_descriptions(self) -> Dict[str, str]:
        """–û–ø–∏—Å–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        return {
            'total_transactions': '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∫–ª–∏–µ–Ω—Ç–∞',
            'total_amount': '–û–±—â–∞—è —Å—É–º–º–∞ –≤—Å–µ—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∫–ª–∏–µ–Ω—Ç–∞ (—Ç–µ–Ω–≥–µ)',
            'avg_amount': '–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ç–µ–Ω–≥–µ)',
            'median_amount': '–ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ç–µ–Ω–≥–µ)',
            'std_amount': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—É–º–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
            'min_amount': '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ç–µ–Ω–≥–µ)',
            'max_amount': '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ç–µ–Ω–≥–µ)',
            'amount_range': '–î–∏–∞–ø–∞–∑–æ–Ω —Å—É–º–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (max - min)',
            'preferred_hour': '–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º–æ–µ –≤—Ä–µ–º—è —Å–æ–≤–µ—Ä—à–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (—á–∞—Å)',
            'preferred_day': '–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (0=–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫)',
            'unique_merchants': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ä—á–∞–Ω—Ç–æ–≤',
            'unique_categories': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π MCC',
            'unique_cities': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤',
            'purchase_count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Ç–∏–ø–∞ "Purchase"',
            'purchase_ratio': '–î–æ–ª—è –ø–æ–∫—É–ø–æ–∫ –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
            'avg_merchants_per_transaction': '–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä—á–∞–Ω—Ç–æ–≤ –Ω–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é',
            'spending_consistency': '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç—Ä–∞—Ç (–æ–±—Ä–∞—Ç–Ω–∞—è –∫ –∫–æ—ç—Ñ. –≤–∞—Ä–∏–∞—Ü–∏–∏)',
            'preferred_pos_mode': '–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —Å–ø–æ—Å–æ–± –≤–≤–æ–¥–∞ –∫–∞—Ä—Ç—ã',
            'preferred_wallet': '–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —Ç–∏–ø —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ—à–µ–ª—å–∫–∞',
            'activity_level': '–£—Ä–æ–≤–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–ù–∏–∑–∫–∞—è/–°—Ä–µ–¥–Ω—è—è/–í—ã—Å–æ–∫–∞—è/–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è)',
            'spending_level': '–£—Ä–æ–≤–µ–Ω—å —Ç—Ä–∞—Ç (–ù–∏–∑–∫–∏–π/–°—Ä–µ–¥–Ω–∏–π/–í—ã—Å–æ–∫–∏–π/–ü—Ä–µ–º–∏—É–º)'
        }
    
    def explore_data(self) -> Dict[str, Any]:
        """–ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        print("\n" + "="*50)
        print("üìä –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
        print("="*50)
        
        
        data_info = {
            'shape': self.df.shape,
            'period': (self.df['transaction_timestamp'].min(), self.df['transaction_timestamp'].max()),
            'unique_clients': self.df['card_id'].nunique(),
            'unique_merchants': self.df['merchant_id'].nunique(),
            'total_volume': self.df['transaction_amount_kzt'].sum(),
            'avg_transaction': self.df['transaction_amount_kzt'].mean(),
            'median_transaction': self.df['transaction_amount_kzt'].median()
        }
        
        print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {data_info['shape']}")
        print(f"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {data_info['period'][0]} - {data_info['period'][1]}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {data_info['unique_clients']:,}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ä—á–∞–Ω—Ç–æ–≤: {data_info['unique_merchants']:,}")
        print(f"–û–±—â–∏–π –æ–±–æ—Ä–æ—Ç: {data_info['total_volume']:,.2f} —Ç–µ–Ω–≥–µ")
        print(f"–°—Ä–µ–¥–Ω—è—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏: {data_info['avg_transaction']:.2f} —Ç–µ–Ω–≥–µ")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏: {data_info['median_transaction']:.2f} —Ç–µ–Ω–≥–µ")
        
        
        missing_data = self.df.isnull().sum()
        if missing_data.sum() > 0:
            print("\nüîç –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
            print(missing_data[missing_data > 0])
            data_info['missing_values'] = missing_data[missing_data > 0].to_dict()
        else:
            print("\n‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
            data_info['missing_values'] = {}
        
        return data_info
    
    def create_behavioral_features(self) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∏–µ–Ω—Ç–æ–≤
        
        –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫:
        1. –ú–µ—Ç—Ä–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ - –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞—Ä—Ç—ã
        2. –ú–µ—Ç—Ä–∏–∫–∏ —Å—É–º–º - —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç –ø–æ–∫—É–ø–∞—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
        3. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã - –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
        4. –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è - –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —à–∏—Ä–æ—Ç—É –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –∫–ª–∏–µ–Ω—Ç–∞
        5. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–∞
        6. –†–∞—Å—á–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –æ –ø–æ–≤–µ–¥–µ–Ω–∏–∏
        """
        print("\nüîß –°–æ–∑–¥–∞–µ–º –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤...")
        
        
        self.df['hour'] = self.df['transaction_timestamp'].dt.hour
        self.df['day_of_week'] = self.df['transaction_timestamp'].dt.dayofweek
        self.df['month'] = self.df['transaction_timestamp'].dt.month
        
        
        print("   üìà –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏...")
        client_agg = self.df.groupby('card_id').agg({
            
            'transaction_id': 'count',  
            'transaction_amount_kzt': ['sum', 'mean', 'median', 'std', 'min', 'max'],
            
            
            'hour': lambda x: x.mode().iloc[0] if not x.mode().empty else 12,
            'day_of_week': lambda x: x.mode().iloc[0] if not x.mode().empty else 0,
            
            
            'merchant_id': 'nunique',  
            'mcc_category': 'nunique',  
            'merchant_city': 'nunique',  
            
            
            'transaction_type': lambda x: (x == 'Purchase').sum(),
            
            
            'pos_entry_mode': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown',
            'wallet_type': lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown'
        }).round(2)
        
        
        client_agg.columns = [
            'total_transactions', 'total_amount', 'avg_amount', 'median_amount', 
            'std_amount', 'min_amount', 'max_amount', 'preferred_hour', 
            'preferred_day', 'unique_merchants', 'unique_categories', 
            'unique_cities', 'purchase_count', 'preferred_pos_mode', 'preferred_wallet'
        ]
        
        print("   üßÆ –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")
        
        
        client_agg['amount_range'] = client_agg['max_amount'] - client_agg['min_amount']
        client_agg['purchase_ratio'] = client_agg['purchase_count'] / client_agg['total_transactions']
        client_agg['avg_merchants_per_transaction'] = client_agg['unique_merchants'] / client_agg['total_transactions']
        
        
        client_agg['spending_consistency'] = 1 / (1 + client_agg['std_amount'] / client_agg['avg_amount'].replace(0, 1))
        
        print("   üè∑Ô∏è –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...")
        
        
        
        activity_quantiles = client_agg['total_transactions'].quantile([0.25, 0.5, 0.75])
        client_agg['activity_level'] = pd.cut(
            client_agg['total_transactions'], 
            bins=[0, activity_quantiles[0.25], activity_quantiles[0.5], activity_quantiles[0.75], float('inf')], 
            labels=['–ù–∏–∑–∫–∞—è', '–°—Ä–µ–¥–Ω—è—è', '–í—ã—Å–æ–∫–∞—è', '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è']
        )
        
        
        spending_quantiles = client_agg['total_amount'].quantile([0.25, 0.5, 0.75])
        client_agg['spending_level'] = pd.cut(
            client_agg['total_amount'], 
            bins=[0, spending_quantiles[0.25], spending_quantiles[0.5], spending_quantiles[0.75], float('inf')], 
            labels=['–ù–∏–∑–∫–∏–π', '–°—Ä–µ–¥–Ω–∏–π', '–í—ã—Å–æ–∫–∏–π', '–ü—Ä–µ–º–∏—É–º']
        )
        
        self.client_features = client_agg
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.client_features)} –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å {len(client_agg.columns)} —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏")
        
        return client_agg
    
    def prepare_features_for_clustering(self) -> Tuple[np.ndarray, pd.Index]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        print("\nüéØ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
        
        if self.client_features is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–º–æ—â—å—é create_behavioral_features()")
        
        
        numeric_features = config.features.clustering_features
        
        print(f"   üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(numeric_features)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for i, feature in enumerate(numeric_features, 1):
            description = self.feature_descriptions.get(feature, "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            print(f"      {i:2d}. {feature}: {description}")
        
        
        features_df = self.client_features[numeric_features].fillna(0)
        
        print(f"   üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤...")
        
        isolation_forest = IsolationForest(
            contamination=config.model.outlier_contamination, 
            random_state=config.model.random_state
        )
        outlier_mask = isolation_forest.fit_predict(features_df) == 1
        
        outlier_count = (~outlier_mask).sum()
        outlier_percentage = (~outlier_mask).mean() * 100
        print(f"   üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {outlier_count} –≤—ã–±—Ä–æ—Å–æ–≤ ({outlier_percentage:.1f}%)")
        
        
        features_clean = features_df[outlier_mask]
        
        print(f"   ‚öñÔ∏è –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        scaler = RobustScaler()
        scaled_features = scaler.fit_transform(features_clean)
        
        self.processed_features = {
            'scaled_data': scaled_features,
            'feature_names': numeric_features,
            'scaler': scaler,
            'clean_indices': features_clean.index,
            'outlier_info': {
                'total_outliers': outlier_count,
                'outlier_percentage': outlier_percentage,
                'contamination_threshold': config.model.outlier_contamination
            }
        }
        
        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {scaled_features.shape[0]} –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        print(f"   üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {scaled_features.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return scaled_features, features_clean.index
    
    def get_feature_importance_analysis(self) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if self.processed_features is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        
        
        feature_stats = pd.DataFrame({
            'feature': self.processed_features['feature_names'],
            'description': [self.feature_descriptions.get(f, '') for f in self.processed_features['feature_names']]
        })
        
        
        clean_indices = self.processed_features['clean_indices']
        original_data = self.client_features.loc[clean_indices, self.processed_features['feature_names']]
        
        feature_stats['mean'] = original_data.mean().values
        feature_stats['std'] = original_data.std().values
        feature_stats['min'] = original_data.min().values
        feature_stats['max'] = original_data.max().values
        feature_stats['coefficient_of_variation'] = (feature_stats['std'] / feature_stats['mean']).round(3)
        
        return feature_stats.round(2)
    
    def save_feature_analysis(self, output_path: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if output_path is None:
            output_path = config.get_output_path('feature_analysis.csv')
        
        feature_analysis = self.get_feature_importance_analysis()
        feature_analysis.to_csv(output_path, index=False, encoding='utf-8')
        print(f"üìä –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        
        return feature_analysis 