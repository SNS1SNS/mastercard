"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any
import matplotlib
matplotlib.use('Agg')  
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.decomposition import PCA
import warnings
import json
warnings.filterwarnings('ignore')

from config import config

class SegmentAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, client_features: pd.DataFrame, cluster_labels: np.ndarray, 
                 scaled_features: np.ndarray, clean_indices: pd.Index):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        self.client_features = client_features
        self.cluster_labels = cluster_labels
        self.scaled_features = scaled_features
        self.clean_indices = clean_indices
        
        
        self.cluster_df = self._create_cluster_dataframe()
        self.segment_profiles = {}
        self.business_recommendations = {}
        
    def _create_cluster_dataframe(self) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        
        cluster_df = self.client_features.loc[self.clean_indices].copy()
        cluster_df['cluster'] = self.cluster_labels
        
        return cluster_df
    
    def analyze_segments(self) -> Dict[int, Dict[str, Any]]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        print("\nüìà –ê–ù–ê–õ–ò–ó –°–ï–ì–ú–ï–ù–¢–û–í –ö–õ–ò–ï–ù–¢–û–í")
        print("="*50)
        
        segment_analysis = {}
        
        
        total_clients = len(self.cluster_df)
        cluster_sizes = self.cluster_df['cluster'].value_counts().sort_index()
        
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {total_clients:,}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(cluster_sizes)}")
        
        
        for cluster_id in sorted(self.cluster_df['cluster'].unique()):
            cluster_data = self.cluster_df[self.cluster_df['cluster'] == cluster_id]
            size = len(cluster_data)
            percentage = (size / total_clients) * 100
            
            print(f"\nüéØ –°–ï–ì–ú–ï–ù–¢ {cluster_id}:")
            print(f"   –†–∞–∑–º–µ—Ä: {size:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")
            
            
            characteristics = self._calculate_segment_characteristics(cluster_data)
            
            
            segment_type = self._classify_segment(cluster_data, characteristics)
            
            
            time_patterns = self._analyze_time_patterns(cluster_data)
            
            
            tech_preferences = self._analyze_tech_preferences(cluster_data)
            
            segment_analysis[cluster_id] = {
                'size': size,
                'percentage': percentage,
                'characteristics': characteristics,
                'segment_type': segment_type,
                'time_patterns': time_patterns,
                'tech_preferences': tech_preferences
            }
            
            
            print(f"   –¢–∏–ø —Å–µ–≥–º–µ–Ω—Ç–∞: {segment_type['name']}")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {characteristics['avg_transactions']:.1f}")
            print(f"   –°—Ä–µ–¥–Ω—è—è –æ–±—â–∞—è —Å—É–º–º–∞: {characteristics['avg_total_amount']:,.0f} —Ç–µ–Ω–≥–µ")
            print(f"   –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: {characteristics['avg_amount']:,.0f} —Ç–µ–Ω–≥–µ")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä—á–∞–Ω—Ç–æ–≤: {characteristics['avg_merchants']:.1f}")
            print(f"   –î–æ–ª—è –ø–æ–∫—É–ø–æ–∫: {characteristics['purchase_ratio']:.2f}")
        
        self.segment_profiles = segment_analysis
        return segment_analysis
    
    def _calculate_segment_characteristics(self, cluster_data: pd.DataFrame) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        return {
            'avg_transactions': cluster_data['total_transactions'].mean(),
            'median_transactions': cluster_data['total_transactions'].median(),
            'avg_total_amount': cluster_data['total_amount'].mean(),
            'median_total_amount': cluster_data['total_amount'].median(),
            'avg_amount': cluster_data['avg_amount'].mean(),
            'median_amount': cluster_data['median_amount'].mean(),
            'avg_merchants': cluster_data['unique_merchants'].mean(),
            'avg_categories': cluster_data['unique_categories'].mean(),
            'avg_cities': cluster_data['unique_cities'].mean(),
            'purchase_ratio': cluster_data['purchase_ratio'].mean(),
            'spending_consistency': cluster_data['spending_consistency'].mean(),
            'amount_range': cluster_data['amount_range'].mean()
        }
    
    def _classify_segment(self, cluster_data: pd.DataFrame, characteristics: Dict[str, float]) -> Dict[str, str]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        
        overall_avg_transactions = self.cluster_df['total_transactions'].mean()
        overall_avg_amount = self.cluster_df['avg_amount'].mean()
        overall_avg_total = self.cluster_df['total_amount'].mean()
        
        avg_transactions = characteristics['avg_transactions']
        avg_amount = characteristics['avg_amount']
        avg_total = characteristics['avg_total_amount']
        
        
        if avg_transactions > overall_avg_transactions * 1.2:  
            if avg_amount > overall_avg_amount * 1.2:  
                return {
                    'name': 'üåü –ü—Ä–µ–º–∏—É–º –∫–ª–∏–µ–Ω—Ç—ã',
                    'description': '–í—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å + –≤—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫',
                    'priority': '–í—ã—Å–æ–∫–∏–π',
                    'value': '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è'
                }
            else:  
                return {
                    'name': '‚ö° –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã',
                    'description': '–í—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å + —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫',
                    'priority': '–í—ã—Å–æ–∫–∏–π',
                    'value': '–í—ã—Å–æ–∫–∞—è'
                }
        else:  
            if avg_amount > overall_avg_amount * 1.2:  
                return {
                    'name': 'üíé VIP –∫–ª–∏–µ–Ω—Ç—ã',
                    'description': '–ù–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å + –≤—ã—Å–æ–∫–∏–π —á–µ–∫',
                    'priority': '–°—Ä–µ–¥–Ω–∏–π',
                    'value': '–í—ã—Å–æ–∫–∞—è'
                }
            elif avg_transactions < overall_avg_transactions * 0.5:  
                return {
                    'name': 'üò¥ –°–ø—è—â–∏–µ –∫–ª–∏–µ–Ω—Ç—ã',
                    'description': '–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å + –Ω–∏–∑–∫–∏–π —á–µ–∫',
                    'priority': '–ù–∏–∑–∫–∏–π',
                    'value': '–ù–∏–∑–∫–∞—è'
                }
            else:  
                return {
                    'name': 'üîÑ –û–±—ã—á–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã',
                    'description': '–°—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å + —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫',
                    'priority': '–°—Ä–µ–¥–Ω–∏–π',
                    'value': '–°—Ä–µ–¥–Ω—è—è'
                }
    
    def _analyze_time_patterns(self, cluster_data: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        return {
            'preferred_hour': cluster_data['preferred_hour'].mode().iloc[0] if not cluster_data['preferred_hour'].mode().empty else 12,
            'preferred_day': cluster_data['preferred_day'].mode().iloc[0] if not cluster_data['preferred_day'].mode().empty else 0,
            'hour_distribution': cluster_data['preferred_hour'].value_counts().to_dict(),
            'day_distribution': cluster_data['preferred_day'].value_counts().to_dict()
        }
    
    def _analyze_tech_preferences(self, cluster_data: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π"""
        return {
            'preferred_pos_mode': cluster_data['preferred_pos_mode'].mode().iloc[0] if not cluster_data['preferred_pos_mode'].mode().empty else 'Unknown',
            'preferred_wallet': cluster_data['preferred_wallet'].mode().iloc[0] if not cluster_data['preferred_wallet'].mode().empty else 'Unknown',
            'pos_distribution': cluster_data['preferred_pos_mode'].value_counts().to_dict(),
            'wallet_distribution': cluster_data['preferred_wallet'].value_counts().to_dict()
        }
    
    def generate_segment_names(self) -> Dict[int, str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        segment_names = {}
        
        for cluster_id, profile in self.segment_profiles.items():
            characteristics = profile['characteristics']
            segment_type = profile['segment_type']
            
            
            base_name = segment_type['name']
            
            
            if characteristics['avg_merchants'] > self.cluster_df['unique_merchants'].mean() * 1.5:
                base_name += " (–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏)"
            elif characteristics['spending_consistency'] > 0.8:
                base_name += " (–°—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ç—Ä–∞—Ç—ã)"
            elif characteristics['avg_cities'] > self.cluster_df['unique_cities'].mean() * 1.5:
                base_name += " (–ü—É—Ç–µ—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–∏)"
            
            segment_names[cluster_id] = base_name
        
        return segment_names
    
    def create_comprehensive_visualizations(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
        print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
        
        
        self._create_interactive_dashboard()
        
        
        self._create_static_visualizations()
        
        
        self._create_pca_visualization()
        
        
        self._create_heatmap()
        
        print("‚úÖ –í—Å–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")
    
    def _create_interactive_dashboard(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞"""
        
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=(
                '–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º',
                '–°—Ä–µ–¥–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤',
                '–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å vs –°—Ä–µ–¥–Ω–∏–π —á–µ–∫',
                '–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è',
                '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è',
                '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è'
            ),
            specs=[[{"type": "pie"}, {"type": "bar"}],
                   [{"type": "scatter"}, {"type": "bar"}],
                   [{"type": "bar"}, {"type": "bar"}]]
        )
        
        
        cluster_sizes = self.cluster_df['cluster'].value_counts().sort_index()
        segment_names = self.generate_segment_names()
        
        fig.add_trace(
            go.Pie(
                labels=[segment_names.get(i, f'–°–µ–≥–º–µ–Ω—Ç {i}') for i in cluster_sizes.index],
                values=cluster_sizes.values,
                name="–†–∞–∑–º–µ—Ä—ã —Å–µ–≥–º–µ–Ω—Ç–æ–≤"
            ),
            row=1, col=1
        )
        
        
        avg_transactions = self.cluster_df.groupby('cluster')['total_transactions'].mean()
        fig.add_trace(
            go.Bar(
                x=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_transactions.index],
                y=avg_transactions.values,
                name='–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π'
            ),
            row=1, col=2
        )
        
        
        avg_amounts = self.cluster_df.groupby('cluster')['avg_amount'].mean()
        cluster_sizes_for_scatter = self.cluster_df.groupby('cluster').size()
        
        fig.add_trace(
            go.Scatter(
                x=avg_transactions.values,
                y=avg_amounts.values,
                mode='markers+text',
                text=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_transactions.index],
                textposition="top center",
                marker=dict(
                    size=cluster_sizes_for_scatter.values / 50,
                    opacity=0.7
                ),
                name='–°–µ–≥–º–µ–Ω—Ç—ã'
            ),
            row=2, col=1
        )
        
        
        avg_merchants = self.cluster_df.groupby('cluster')['unique_merchants'].mean()
        fig.add_trace(
            go.Bar(
                x=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_merchants.index],
                y=avg_merchants.values,
                name='–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä—á–∞–Ω—Ç–æ–≤'
            ),
            row=2, col=2
        )
        
        
        preferred_hours = self.cluster_df.groupby('cluster')['preferred_hour'].mean()
        fig.add_trace(
            go.Bar(
                x=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in preferred_hours.index],
                y=preferred_hours.values,
                name='–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —á–∞—Å'
            ),
            row=3, col=1
        )
        
        
        
        purchase_ratios = self.cluster_df.groupby('cluster')['purchase_ratio'].mean()
        fig.add_trace(
            go.Bar(
                x=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in purchase_ratios.index],
                y=purchase_ratios.values,
                name='–î–æ–ª—è –ø–æ–∫—É–ø–æ–∫'
            ),
            row=3, col=2
        )
        
        
        fig.update_layout(
            title_text="–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤",
            title_x=0.5,
            height=1200,
            showlegend=False
        )
        
        
        output_path = config.get_output_path('segment_dashboard.html')
        fig.write_html(output_path)
        print(f"üìä –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    
    def _create_static_visualizations(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
        plt.style.use(config.visualization.style)
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('–ê–Ω–∞–ª–∏–∑ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤', fontsize=16, fontweight='bold')
        
        
        cluster_sizes = self.cluster_df['cluster'].value_counts().sort_index()
        axes[0, 0].pie(cluster_sizes.values, labels=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in cluster_sizes.index], 
                      autopct='%1.1f%%', startangle=90)
        axes[0, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º')
        
        
        avg_transactions = self.cluster_df.groupby('cluster')['total_transactions'].mean()
        axes[0, 1].bar(range(len(avg_transactions)), avg_transactions.values)
        axes[0, 1].set_title('–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π')
        axes[0, 1].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[0, 1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π')
        axes[0, 1].set_xticks(range(len(avg_transactions)))
        axes[0, 1].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_transactions.index])
        
        
        avg_amounts = self.cluster_df.groupby('cluster')['total_amount'].mean()
        axes[0, 2].bar(range(len(avg_amounts)), avg_amounts.values)
        axes[0, 2].set_title('–°—Ä–µ–¥–Ω—è—è –æ–±—â–∞—è —Å—É–º–º–∞')
        axes[0, 2].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[0, 2].set_ylabel('–°—É–º–º–∞ (—Ç–µ–Ω–≥–µ)')
        axes[0, 2].set_xticks(range(len(avg_amounts)))
        axes[0, 2].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_amounts.index])
        
        
        avg_check = self.cluster_df.groupby('cluster')['avg_amount'].mean()
        axes[1, 0].bar(range(len(avg_check)), avg_check.values)
        axes[1, 0].set_title('–°—Ä–µ–¥–Ω–∏–π —á–µ–∫')
        axes[1, 0].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[1, 0].set_ylabel('–°—Ä–µ–¥–Ω–∏–π —á–µ–∫ (—Ç–µ–Ω–≥–µ)')
        axes[1, 0].set_xticks(range(len(avg_check)))
        axes[1, 0].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_check.index])
        
        
        avg_merchants = self.cluster_df.groupby('cluster')['unique_merchants'].mean()
        axes[1, 1].bar(range(len(avg_merchants)), avg_merchants.values)
        axes[1, 1].set_title('–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä—á–∞–Ω—Ç–æ–≤')
        axes[1, 1].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[1, 1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ä—á–∞–Ω—Ç–æ–≤')
        axes[1, 1].set_xticks(range(len(avg_merchants)))
        axes[1, 1].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in avg_merchants.index])
        
        
        axes[1, 2].scatter(avg_transactions.values, avg_check.values, 
                          s=cluster_sizes.values/10, alpha=0.7)
        for i, (x, y) in enumerate(zip(avg_transactions.values, avg_check.values)):
            axes[1, 2].annotate(f'–°–µ–≥–º–µ–Ω—Ç {avg_transactions.index[i]}', (x, y), 
                               xytext=(5, 5), textcoords='offset points')
        axes[1, 2].set_title('–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å vs –°—Ä–µ–¥–Ω–∏–π —á–µ–∫')
        axes[1, 2].set_xlabel('–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π')
        axes[1, 2].set_ylabel('–°—Ä–µ–¥–Ω–∏–π —á–µ–∫ (—Ç–µ–Ω–≥–µ)')
        
        
        preferred_hours = self.cluster_df.groupby('cluster')['preferred_hour'].mean()
        axes[2, 0].bar(range(len(preferred_hours)), preferred_hours.values)
        axes[2, 0].set_title('–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º–æ–µ –≤—Ä–µ–º—è (—á–∞—Å)')
        axes[2, 0].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[2, 0].set_ylabel('–ß–∞—Å')
        axes[2, 0].set_xticks(range(len(preferred_hours)))
        axes[2, 0].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in preferred_hours.index])
        
        
        purchase_ratios = self.cluster_df.groupby('cluster')['purchase_ratio'].mean()
        axes[2, 1].bar(range(len(purchase_ratios)), purchase_ratios.values)
        axes[2, 1].set_title('–î–æ–ª—è –ø–æ–∫—É–ø–æ–∫')
        axes[2, 1].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[2, 1].set_ylabel('–î–æ–ª—è –ø–æ–∫—É–ø–æ–∫')
        axes[2, 1].set_xticks(range(len(purchase_ratios)))
        axes[2, 1].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in purchase_ratios.index])
        
        
        spending_consistency = self.cluster_df.groupby('cluster')['spending_consistency'].mean()
        axes[2, 2].bar(range(len(spending_consistency)), spending_consistency.values)
        axes[2, 2].set_title('–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç—Ä–∞—Ç')
        axes[2, 2].set_xlabel('–°–µ–≥–º–µ–Ω—Ç')
        axes[2, 2].set_ylabel('–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å')
        axes[2, 2].set_xticks(range(len(spending_consistency)))
        axes[2, 2].set_xticklabels([f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in spending_consistency.index])
        
        plt.tight_layout()
        output_path = config.get_output_path('segment_analysis_static.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print("‚úÖ –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def _create_pca_visualization(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        
        pca = PCA(n_components=2, random_state=config.model.random_state)
        pca_features = pca.fit_transform(self.scaled_features)
        
        plt.figure(figsize=(12, 8))
        
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(np.unique(self.cluster_labels))))
        
        for i, cluster_id in enumerate(sorted(np.unique(self.cluster_labels))):
            mask = self.cluster_labels == cluster_id
            plt.scatter(pca_features[mask, 0], pca_features[mask, 1], 
                       c=[colors[i]], label=f'–°–µ–≥–º–µ–Ω—Ç {cluster_id}', 
                       alpha=0.6, s=50)
        
        plt.title('–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ PCA')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} –¥–∏—Å–ø–µ—Ä—Å–∏–∏)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} –¥–∏—Å–ø–µ—Ä—Å–∏–∏)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        output_path = config.get_output_path('pca_visualization.png')
        plt.savefig(output_path, dpi=config.visualization.dpi, bbox_inches='tight')
        plt.close()
        
        print(f"üìä PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
        print(f"   –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è: PC1={pca.explained_variance_ratio_[0]:.1%}, PC2={pca.explained_variance_ratio_[1]:.1%}")
    
    def _create_heatmap(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        
        key_metrics = [
            'total_transactions', 'total_amount', 'avg_amount', 
            'unique_merchants', 'unique_categories', 'purchase_ratio',
            'spending_consistency'
        ]
        
        try:
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            heatmap_data = self.cluster_df.groupby('cluster')[key_metrics].mean()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
            if heatmap_data.empty:
                print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã")
                return
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN
            heatmap_normalized = heatmap_data.copy()
            for col in heatmap_data.columns:
                col_min = heatmap_data[col].min()
                col_max = heatmap_data[col].max()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ä–∞–∑–±—Ä–æ—Å –≤ –¥–∞–Ω–Ω—ã—Ö
                if col_max != col_min and not pd.isna(col_min) and not pd.isna(col_max):
                    heatmap_normalized[col] = (heatmap_data[col] - col_min) / (col_max - col_min)
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–±—Ä–æ—Å–∞ –∏–ª–∏ –µ—Å—Ç—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    heatmap_normalized[col] = 0.5
            
            # –ó–∞–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –Ω–∞ 0
            heatmap_normalized = heatmap_normalized.fillna(0.5)
            
            plt.figure(figsize=(12, 8))
            sns.heatmap(heatmap_normalized.T, 
                       annot=True, 
                       cmap='YlOrRd', 
                       xticklabels=[f'–°–µ–≥–º–µ–Ω—Ç {i}' for i in heatmap_data.index],
                       yticklabels=[
                           '–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏', '–û–±—â–∞—è —Å—É–º–º–∞', '–°—Ä–µ–¥–Ω–∏–π —á–µ–∫',
                           '–ú–µ—Ä—á–∞–Ω—Ç—ã', '–ö–∞—Ç–µ–≥–æ—Ä–∏–∏', '–î–æ–ª—è –ø–æ–∫—É–ø–æ–∫',
                           '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å'
                       ],
                       fmt='.2f',
                       cbar_kws={'label': '–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ'})
            
            plt.title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤\n(–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)')
            plt.tight_layout()
            
            output_path = config.get_output_path('segment_heatmap.png')
            plt.savefig(output_path, dpi=config.visualization.dpi, bbox_inches='tight')
            plt.close()
            
            print(f"üìä –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã: {e}")
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –∑–∞–≥–ª—É—à–∫—É
            plt.figure(figsize=(8, 6))
            plt.text(0.5, 0.5, f'–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è\n—Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã:\n{str(e)}', 
                    ha='center', va='center', transform=plt.gca().transAxes,
                    fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
            plt.title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤')
            plt.axis('off')
            
            output_path = config.get_output_path('segment_heatmap.png')
            plt.savefig(output_path, dpi=config.visualization.dpi, bbox_inches='tight')
            plt.close()
            
            print(f"üìä –ó–∞–≥–ª—É—à–∫–∞ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
    
    def generate_business_recommendations(self) -> Dict[int, Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        print("\nüíº –ì–ï–ù–ï–†–ê–¶–ò–Ø –ë–ò–ó–ù–ï–°-–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô")
        print("="*50)
        
        recommendations = {}
        
        for cluster_id, profile in self.segment_profiles.items():
            segment_type = profile['segment_type']
            characteristics = profile['characteristics']
            size_pct = profile['percentage']
            
            print(f"\nüéØ –°–ï–ì–ú–ï–ù–¢ {cluster_id}: {segment_type['name']} ({size_pct:.1f}% –∫–ª–∏–µ–Ω—Ç–æ–≤)")
            
            
            if 'üåü –ü—Ä–µ–º–∏—É–º' in segment_type['name']:
                rec = self._generate_premium_recommendations(characteristics)
            elif '‚ö° –ê–∫—Ç–∏–≤–Ω—ã–µ' in segment_type['name']:
                rec = self._generate_active_recommendations(characteristics)
            elif 'üíé VIP' in segment_type['name']:
                rec = self._generate_vip_recommendations(characteristics)
            elif 'üò¥ –°–ø—è—â–∏–µ' in segment_type['name']:
                rec = self._generate_sleeping_recommendations(characteristics)
            else:  
                rec = self._generate_regular_recommendations(characteristics)
            
            recommendations[cluster_id] = {
                'segment_info': profile,
                'strategies': rec['strategies'],
                'products': rec['products'],
                'channels': rec['channels'],
                'kpis': rec['kpis'],
                'priority': segment_type['priority'],
                'expected_impact': rec['expected_impact']
            }
            
            
            print(f"   üìà –°–¢–†–ê–¢–ï–ì–ò–ò:")
            for strategy in rec['strategies']:
                print(f"     ‚Ä¢ {strategy}")
            
            print(f"   üéÅ –ü–†–û–î–£–ö–¢–´:")
            for product in rec['products']:
                print(f"     ‚Ä¢ {product}")
            
            print(f"   üì± –ö–ê–ù–ê–õ–´:")
            for channel in rec['channels']:
                print(f"     ‚Ä¢ {channel}")
        
        self.business_recommendations = recommendations
        return recommendations
    
    def _generate_premium_recommendations(self, characteristics: Dict[str, float]) -> Dict[str, Any]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–º–∏—É–º –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return {
            'strategies': [
                '–ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ —Å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–º–∏ –ø—Ä–∏–≤–∏–ª–µ–≥–∏—è–º–∏',
                '–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∏ –∫–æ–Ω—Å—å–µ—Ä–∂-—Å–µ—Ä–≤–∏—Å',
                '–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤–æ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–∞—Ö',
                '–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è',
                '–ü–æ–≤—ã—à–µ–Ω–Ω—ã–µ –ª–∏–º–∏—Ç—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è'
            ],
            'products': [
                'Private Banking —É—Å–ª—É–≥–∏',
                '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ –ø–æ—Ä—Ç—Ñ–µ–ª–∏',
                '–ü—Ä–µ–º–∏—É–º –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º –∫—ç—à–±—ç–∫–æ–º',
                'VIP —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∏ –∑–∞—â–∏—Ç–∞',
                '–ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –ø–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∫–∞–ø–∏—Ç–∞–ª–æ–º'
            ],
            'channels': [
                '–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –≤—Å—Ç—Ä–µ—á–∏ —Å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º',
                '–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ª–∏–Ω–∏—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏',
                '–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º',
                '–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ digital-—Å–µ—Ä–≤–∏—Å—ã'
            ],
            'kpis': [
                '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞ –Ω–∞ 15-20%',
                '–†–æ—Å—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –Ω–∞ –∫–ª–∏–µ–Ω—Ç–∞',
                '–ü–æ–≤—ã—à–µ–Ω–∏–µ NPS –¥–æ 80+',
                '–°–Ω–∏–∂–µ–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –¥–æ 2-3%'
            ],
            'expected_impact': '–í—ã—Å–æ–∫–∏–π - –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∏–±—ã–ª–∏ –±–∞–Ω–∫–∞'
        }
    
    def _generate_active_recommendations(self, characteristics: Dict[str, float]) -> Dict[str, Any]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return {
            'strategies': [
                '–ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ —Å –±–æ–Ω—É—Å–∞–º–∏ –∑–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å',
                '–ì–µ–π–º–∏—Ñ–∏–∫–∞—Ü–∏—è –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —É—Å–ª—É–≥',
                '–ö—ç—à–±—ç–∫ –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö',
                '–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —á–∞—Å—Ç—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π',
                '–†–∞–∑–≤–∏—Ç–∏–µ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤'
            ],
            'products': [
                '–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã —Å –ª—å–≥–æ—Ç–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏',
                '–ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —Å—Ç–∞–≤–∫–æ–π',
                '–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –∫—Ä–µ–¥–∏—Ç—ã –Ω–∞ –≤—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö',
                '–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –ø–æ–∫—É–ø–æ–∫ –∏ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π',
                '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö'
            ],
            'channels': [
                '–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞–Ω–∞–ª',
                'Push-—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö',
                '–ß–∞—Ç-–±–æ—Ç –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π',
                '–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏'
            ],
            'kpis': [
                '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞ 25%',
                '–†–æ—Å—Ç cross-sell –Ω–∞ 30%',
                '–ü–æ–≤—ã—à–µ–Ω–∏–µ engagement –≤ digital-–∫–∞–Ω–∞–ª–∞—Ö',
                'NPS 70+'
            ],
            'expected_impact': '–í—ã—Å–æ–∫–∏–π - –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞ –≤ –ø—Ä–µ–º–∏—É–º —Å–µ–≥–º–µ–Ω—Ç'
        }
    
    def _generate_vip_recommendations(self, characteristics: Dict[str, float]) -> Dict[str, Any]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è VIP –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return {
            'strategies': [
                '–°—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞—Ä—Ç',
                '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è',
                '–ö–æ–Ω—Å—å–µ—Ä–∂-—Å–µ—Ä–≤–∏—Å—ã –∏ lifestyle-—É—Å–ª—É–≥–∏',
                '–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –∏ –ø—Ä–∏–≤–∏–ª–µ–≥–∏–∏',
                '–ü—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏'
            ],
            'products': [
                '–ü—Ä–µ–º–∏—É–º –∫–∞—Ä—Ç—ã —Å –æ—Å–æ–±—ã–º–∏ –ø—Ä–∏–≤–∏–ª–µ–≥–∏—è–º–∏',
                '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏ –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã',
                '–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–º–∏—É–º-–∫–ª–∞—Å—Å–∞',
                '–£—Å–ª—É–≥–∏ private banking',
                '–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –ø–æ–¥ –∑–∞–ª–æ–≥ –∞–∫—Ç–∏–≤–æ–≤'
            ],
            'channels': [
                '–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏',
                '–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞',
                '–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è',
                '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ digital-—Å–µ—Ä–≤–∏—Å—ã'
            ],
            'kpis': [
                '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –Ω–∞ 40%',
                '–†–æ—Å—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ 2 —Ä–∞–∑–∞',
                '–ü–æ–≤—ã—à–µ–Ω–∏–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏',
                '–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∏—Å–∫–∞ –æ—Ç—Ç–æ–∫–∞'
            ],
            'expected_impact': '–°—Ä–µ–¥–Ω–∏–π - –≤—ã—Å–æ–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏'
        }
    
    def _generate_sleeping_recommendations(self, characteristics: Dict[str, float]) -> Dict[str, Any]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å–ø—è—â–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return {
            'strategies': [
                '–†–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏',
                '–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç–∏',
                '–ü—Ä–æ—Å—Ç—ã–µ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã',
                '–°—Ç–∏–º—É–ª–∏—Ä—É—é—â–∏–µ –∞–∫—Ü–∏–∏ –¥–ª—è –ø–µ—Ä–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
                '–£–ø—Ä–æ—â–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è'
            ],
            'products': [
                '–ë–∞–∑–æ–≤—ã–µ –¥–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –±–µ–∑ –∫–æ–º–∏—Å—Å–∏–π',
                '–ü—Ä–æ—Å—Ç—ã–µ –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞',
                '–ú–∏–∫—Ä–æ–∫—Ä–µ–¥–∏—Ç—ã –∏ —Ä–∞—Å—Å—Ä–æ—á–∫–∏',
                '–ë–∞–∑–æ–≤–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ',
                '–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã'
            ],
            'channels': [
                'SMS –∏ email-—Ä–∞—Å—Å—ã–ª–∫–∏',
                '–ü—Ä–æ—Å—Ç–æ–µ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',
                '–û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –≤–µ–±–∏–Ω–∞—Ä—ã',
                '–ö–æ–Ω—Ç–∞–∫—Ç-—Ü–µ–Ω—Ç—Ä –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏'
            ],
            'kpis': [
                '–†–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è 15-20% –∫–ª–∏–µ–Ω—Ç–æ–≤',
                '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ 3-5 —Ä–∞–∑',
                '–°–Ω–∏–∂–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è',
                '–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç–∏'
            ],
            'expected_impact': '–ù–∏–∑–∫–∏–π - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ —Ä–∞–∑–≤–∏—Ç–∏—è'
        }
    
    def _generate_regular_recommendations(self, characteristics: Dict[str, float]) -> Dict[str, Any]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return {
            'strategies': [
                '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏',
                'Cross-sell –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤',
                '–†–∞–∑–≤–∏—Ç–∏–µ digital-–ø—Ä–∏–≤—ã—á–µ–∫',
                '–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏',
                '–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –±–∞–∑–æ–≤–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'
            ],
            'products': [
                '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã',
                '–î–µ–ø–æ–∑–∏—Ç—ã –∏ –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞',
                '–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –∫—Ä–µ–¥–∏—Ç—ã',
                '–ë–∞–∑–æ–≤–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ',
                '–ü–ª–∞—Ç–µ–∂–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã'
            ],
            'channels': [
                '–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',
                '–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫',
                '–û—Ç–¥–µ–ª–µ–Ω–∏—è –±–∞–Ω–∫–∞',
                '–ö–æ–Ω—Ç–∞–∫—Ç-—Ü–µ–Ω—Ç—Ä'
            ],
            'kpis': [
                '–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –Ω–∞ 1-2',
                '–†–æ—Å—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞ 20%',
                '–ü–æ–≤—ã—à–µ–Ω–∏–µ NPS –¥–æ 60',
                '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤'
            ],
            'expected_impact': '–°—Ä–µ–¥–Ω–∏–π - —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –±–∞–∑–∞ –±–∞–Ω–∫–∞'
        }
    
    def save_detailed_analysis(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞...")
        
        
        output_path = config.get_output_path(config.output.segments_file)
        self.cluster_df.to_csv(output_path, index=True, encoding='utf-8')
        print(f"   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {output_path}")
        
        
        summary_stats = self.cluster_df.groupby('cluster').agg({
            'total_transactions': ['count', 'mean', 'median', 'std'],
            'total_amount': ['mean', 'median', 'std'],
            'avg_amount': ['mean', 'median', 'std'],
            'unique_merchants': ['mean', 'median'],
            'unique_categories': ['mean', 'median'],
            'purchase_ratio': ['mean', 'median'],
            'spending_consistency': ['mean', 'median']
        }).round(2)
        
        summary_path = config.get_output_path(config.output.summary_file)
        summary_stats.to_csv(summary_path, encoding='utf-8')
        print(f"   üìà –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {summary_path}")
        
        
        profiles_path = config.get_output_path('segment_profiles.json')
        with open(profiles_path, 'w', encoding='utf-8') as f:
            
            profiles_json = {}
            for k, v in self.segment_profiles.items():
                profiles_json[str(k)] = self._convert_for_json(v)
            json.dump(profiles_json, f, ensure_ascii=False, indent=2)
        print(f"   üë• –ü—Ä–æ—Ñ–∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {profiles_path}")
        
        
        recommendations_path = config.get_output_path('business_recommendations.json')
        with open(recommendations_path, 'w', encoding='utf-8') as f:
            recommendations_json = {}
            for k, v in self.business_recommendations.items():
                recommendations_json[str(k)] = self._convert_for_json(v)
            json.dump(recommendations_json, f, ensure_ascii=False, indent=2)
        print(f"   üíº –ë–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {recommendations_path}")
        
        
        print(f"\nüî¨ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏...")
        
        
        stability_metrics = self.analyze_segment_stability()
        stability_path = config.get_output_path('segment_stability.json')
        with open(stability_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_for_json(stability_metrics), f, ensure_ascii=False, indent=2)
        print(f"   üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏: {stability_path}")
        
        
        clv_metrics = self.calculate_customer_lifetime_value()
        clv_path = config.get_output_path('customer_lifetime_value.json')
        with open(clv_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_for_json(clv_metrics), f, ensure_ascii=False, indent=2)
        print(f"   üí∞ Customer Lifetime Value: {clv_path}")
        
        
        journey_analysis = self.analyze_customer_journey()
        journey_path = config.get_output_path('customer_journey.json')
        with open(journey_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_for_json(journey_analysis), f, ensure_ascii=False, indent=2)
        print(f"   üõ§Ô∏è –ê–Ω–∞–ª–∏–∑ –ø—É—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞: {journey_path}")
        
        
        monitoring_data = self.create_monitoring_dashboard_data()
        print(f"   üìä –î–∞–Ω–Ω—ã–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ–∑–¥–∞–Ω—ã")
        
        
        ab_tests = self.design_ab_test_framework()
        print(f"   üß™ –ü–ª–∞–Ω A/B —Ç–µ—Å—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω")
        
        
        print(f"\nüìã –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –∏ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤...")
        
        
        executive_report = self.generate_executive_report()
        print(f"   üìã –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω")
        
        
        presentation_data = self.create_presentation_slides()
        print(f"   üé® –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã")
        
        print("‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def _convert_for_json(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if isinstance(obj, dict):
            return {str(k): self._convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_for_json(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif hasattr(obj, 'item'):  
            return obj.item()
        else:
            return obj
    
    def print_executive_summary(self):
        """–ü–µ—á–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞"""
        print("\n" + "="*60)
        print("üìã –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï –î–õ–Ø –†–£–ö–û–í–û–î–°–¢–í–ê")
        print("="*60)
        
        total_clients = len(self.cluster_df)
        n_segments = len(self.segment_profiles)
        
        print(f"üéØ –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   ‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {total_clients:,}")
        print(f"   ‚Ä¢ –í—ã—è–≤–ª–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {n_segments}")
        
        print(f"\nüìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π –±–∞–∑—ã:")
        
        
        segments_by_size = sorted(self.segment_profiles.items(), 
                                key=lambda x: x[1]['percentage'], reverse=True)
        
        for cluster_id, profile in segments_by_size:
            segment_type = profile['segment_type']
            print(f"   ‚Ä¢ {segment_type['name']}: {profile['percentage']:.1f}% "
                  f"({profile['size']:,} –∫–ª–∏–µ–Ω—Ç–æ–≤) - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {segment_type['priority']}")
        
        print(f"\nüí° –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã:")
        
        
        most_valuable = max(segments_by_size, 
                          key=lambda x: x[1]['characteristics']['avg_total_amount'])
        print(f"   ‚Ä¢ –°–∞–º—ã–π —Ü–µ–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: {most_valuable[1]['segment_type']['name']} "
              f"(—Å—Ä–µ–¥–Ω–∏–π –æ–±–æ—Ä–æ—Ç: {most_valuable[1]['characteristics']['avg_total_amount']:,.0f} —Ç–µ–Ω–≥–µ)")
        
        
        most_active = max(segments_by_size, 
                         key=lambda x: x[1]['characteristics']['avg_transactions'])
        print(f"   ‚Ä¢ –°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: {most_active[1]['segment_type']['name']} "
              f"(—Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {most_active[1]['characteristics']['avg_transactions']:.1f})")
        
        
        growth_potential = [s for s in segments_by_size 
                          if '–°–ø—è—â–∏–µ' in s[1]['segment_type']['name'] or 
                             'VIP' in s[1]['segment_type']['name']]
        if growth_potential:
            print(f"   ‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞: {growth_potential[0][1]['segment_type']['name']} "
                  f"({growth_potential[0][1]['percentage']:.1f}% –∫–ª–∏–µ–Ω—Ç–æ–≤)")
        
        print(f"\nüéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏:")
        print(f"   1. –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ü—Ä–µ–º–∏—É–º –∏ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–∫—É—Å)")
        print(f"   2. –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: VIP –∏ –û–±—ã—á–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã (—Ä–∞–∑–≤–∏—Ç–∏–µ –∏ —É–¥–µ—Ä–∂–∞–Ω–∏–µ)")
        print(f"   3. –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°–ø—è—â–∏–µ –∫–ª–∏–µ–Ω—Ç—ã (–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã)")
        
        print("="*60)
    
    def analyze_segment_stability(self) -> Dict[int, Dict[str, float]]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏"""
        print("\nüìä –ê–ù–ê–õ–ò–ó –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò –°–ï–ì–ú–ï–ù–¢–û–í")
        print("="*50)
        
        stability_metrics = {}
        
        for cluster_id in sorted(self.cluster_df['cluster'].unique()):
            cluster_data = self.cluster_df[self.cluster_df['cluster'] == cluster_id]
            
            
            cv_transactions = cluster_data['total_transactions'].std() / cluster_data['total_transactions'].mean()
            cv_amount = cluster_data['avg_amount'].std() / cluster_data['avg_amount'].mean()
            cv_merchants = cluster_data['unique_merchants'].std() / cluster_data['unique_merchants'].mean()
            
            
            stability_index = (cv_transactions + cv_amount + cv_merchants) / 3
            
            
            low_activity_threshold = self.cluster_df['total_transactions'].quantile(0.25)
            churn_risk = len(cluster_data[cluster_data['total_transactions'] < low_activity_threshold]) / len(cluster_data)
            
            stability_metrics[cluster_id] = {
                'stability_index': stability_index,
                'cv_transactions': cv_transactions,
                'cv_amount': cv_amount,
                'cv_merchants': cv_merchants,
                'churn_risk': churn_risk,
                'consistency_score': 1 - stability_index  
            }
            
            print(f"\nüéØ –°–ï–ì–ú–ï–ù–¢ {cluster_id}:")
            print(f"   –ò–Ω–¥–µ–∫—Å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏: {stability_index:.3f} ({'–°—Ç–∞–±–∏–ª—å–Ω—ã–π' if stability_index < 0.5 else '–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π'})")
            print(f"   –†–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞: {churn_risk:.1%}")
            print(f"   –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {1 - stability_index:.3f}")
        
        return stability_metrics
    
    def calculate_customer_lifetime_value(self) -> Dict[int, Dict[str, float]]:
        """–†–∞—Å—á–µ—Ç Customer Lifetime Value –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        print("\nüí∞ –†–ê–°–ß–ï–¢ CUSTOMER LIFETIME VALUE")
        print("="*50)
        
        clv_metrics = {}
        
        for cluster_id in sorted(self.cluster_df['cluster'].unique()):
            cluster_data = self.cluster_df[self.cluster_df['cluster'] == cluster_id]
            
            
            avg_monthly_revenue = cluster_data['total_amount'].mean() / 12  
            
            
            avg_transaction_frequency = cluster_data['total_transactions'].mean()
            
            
            
            if avg_transaction_frequency > 0:
                estimated_lifetime_months = min(60, max(12, avg_transaction_frequency * 2))  
            else:
                estimated_lifetime_months = 12
            
            
            clv = avg_monthly_revenue * estimated_lifetime_months
            
            
            stability_data = self.analyze_segment_stability()
            churn_risk = stability_data.get(cluster_id, {}).get('churn_risk', 0.5)
            adjusted_clv = clv * (1 - churn_risk)
            
            clv_metrics[cluster_id] = {
                'avg_monthly_revenue': avg_monthly_revenue,
                'estimated_lifetime_months': estimated_lifetime_months,
                'basic_clv': clv,
                'adjusted_clv': adjusted_clv,
                'churn_risk': churn_risk,
                'value_tier': self._classify_clv_tier(adjusted_clv)
            }
            
            print(f"\nüéØ –°–ï–ì–ú–ï–ù–¢ {cluster_id}:")
            print(f"   –°—Ä–µ–¥–Ω–∏–π –º–µ—Å—è—á–Ω—ã–π –¥–æ—Ö–æ–¥: {avg_monthly_revenue:,.0f} —Ç–µ–Ω–≥–µ")
            print(f"   –û—Ü–µ–Ω–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –∂–∏–∑–Ω–∏: {estimated_lifetime_months:.1f} –º–µ—Å—è—Ü–µ–≤")
            print(f"   CLV (–±–∞–∑–æ–≤—ã–π): {clv:,.0f} —Ç–µ–Ω–≥–µ")
            print(f"   CLV (—Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π): {adjusted_clv:,.0f} —Ç–µ–Ω–≥–µ")
            print(f"   –£—Ä–æ–≤–µ–Ω—å —Ü–µ–Ω–Ω–æ—Å—Ç–∏: {self._classify_clv_tier(adjusted_clv)}")
        
        return clv_metrics
    
    def _classify_clv_tier(self, clv: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω—è CLV"""
        if clv >= 500000:  
            return "üåü –í—ã—Å–æ–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å"
        elif clv >= 200000:  
            return "üíé –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–Ω–æ—Å—Ç—å"
        elif clv >= 50000:   
            return "‚≠ê –ë–∞–∑–æ–≤–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å"
        else:
            return "üìâ –ù–∏–∑–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å"
    
    def analyze_customer_journey(self) -> Dict[int, Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –ø—É—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞ –∏ —ç—Ç–∞–ø–æ–≤ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞"""
        print("\nüõ§Ô∏è –ê–ù–ê–õ–ò–ó –ü–£–¢–ò –ö–õ–ò–ï–ù–¢–ê")
        print("="*50)
        
        journey_analysis = {}
        
        for cluster_id in sorted(self.cluster_df['cluster'].unique()):
            cluster_data = self.cluster_df[self.cluster_df['cluster'] == cluster_id]
            
            
            avg_transactions = cluster_data['total_transactions'].mean()
            avg_merchants = cluster_data['unique_merchants'].mean()
            spending_consistency = cluster_data['spending_consistency'].mean()
            
            
            if avg_transactions < 5 and avg_merchants < 10:
                lifecycle_stage = "üå± –ù–æ–≤–∏—á–∫–∏"
                stage_description = "–¢–æ–ª—å–∫–æ –Ω–∞—á–∏–Ω–∞—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É—Å–ª—É–≥–∞–º–∏"
            elif avg_transactions < 20 and spending_consistency < 0.5:
                lifecycle_stage = "üîç –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏"
                stage_description = "–ò–∑—É—á–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏"
            elif spending_consistency >= 0.7 and avg_merchants >= 20:
                lifecycle_stage = "üí™ –ó—Ä–µ–ª—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"
                stage_description = "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å–ª—É–≥"
            elif avg_transactions >= 50:
                lifecycle_stage = "üëë –ß–µ–º–ø–∏–æ–Ω—ã"
                stage_description = "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å–ª—É–≥"
            else:
                lifecycle_stage = "‚öñÔ∏è –°—Ç–∞–±–∏–ª—å–Ω—ã–µ"
                stage_description = "–†–µ–≥—É–ª—è—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å–ª—É–≥"
            
            
            preferred_time = cluster_data['preferred_hour'].mode().iloc[0] if not cluster_data['preferred_hour'].mode().empty else 12
            time_pattern = self._classify_time_pattern(preferred_time)
            
            
            tech_diversity = cluster_data[['preferred_pos_mode', 'preferred_wallet']].nunique().sum()
            tech_maturity = "–í—ã—Å–æ–∫–∞—è" if tech_diversity > 3 else "–°—Ä–µ–¥–Ω—è—è" if tech_diversity > 1 else "–ù–∏–∑–∫–∞—è"
            
            journey_analysis[cluster_id] = {
                'lifecycle_stage': lifecycle_stage,
                'stage_description': stage_description,
                'time_pattern': time_pattern,
                'tech_maturity': tech_maturity,
                'engagement_level': self._calculate_engagement_level(cluster_data),
                'next_best_action': self._suggest_next_action(lifecycle_stage, cluster_data)
            }
            
            print(f"\nüéØ –°–ï–ì–ú–ï–ù–¢ {cluster_id}:")
            print(f"   –≠—Ç–∞–ø –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞: {lifecycle_stage}")
            print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {stage_description}")
            print(f"   –í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: {time_pattern}")
            print(f"   –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∑—Ä–µ–ª–æ—Å—Ç—å: {tech_maturity}")
        
        return journey_analysis
    
    def _classify_time_pattern(self, hour: int) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        if 6 <= hour <= 9:
            return "üåÖ –£—Ç—Ä–µ–Ω–Ω–∏–µ"
        elif 10 <= hour <= 14:
            return "‚òÄÔ∏è –î–Ω–µ–≤–Ω—ã–µ"
        elif 15 <= hour <= 18:
            return "üåÜ –í–µ—á–µ—Ä–Ω–∏–µ"
        elif 19 <= hour <= 22:
            return "üåô –ù–æ—á–Ω—ã–µ"
        else:
            return "ü¶â –ü–æ–∑–¥–Ω–∏–µ"
    
    def _calculate_engagement_level(self, cluster_data: pd.DataFrame) -> str:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏"""
        
        transactions_score = min(1.0, cluster_data['total_transactions'].mean() / 100)
        merchants_score = min(1.0, cluster_data['unique_merchants'].mean() / 50)
        consistency_score = cluster_data['spending_consistency'].mean()
        
        engagement_score = (transactions_score + merchants_score + consistency_score) / 3
        
        if engagement_score >= 0.8:
            return "üî• –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π"
        elif engagement_score >= 0.6:
            return "‚ö° –í—ã—Å–æ–∫–∏–π"
        elif engagement_score >= 0.4:
            return "üìà –°—Ä–µ–¥–Ω–∏–π"
        else:
            return "üìâ –ù–∏–∑–∫–∏–π"
    
    def _suggest_next_action(self, lifecycle_stage: str, cluster_data: pd.DataFrame) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π"""
        actions = []
        
        if "–ù–æ–≤–∏—á–∫–∏" in lifecycle_stage:
            actions = [
                "–û–Ω–±–æ—Ä–¥–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º–∞",
                "–û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã",
                "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–µ –±–æ–Ω—É—Å—ã",
                "–ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã"
            ]
        elif "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏" in lifecycle_stage:
            actions = [
                "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
                "–î–µ–º–æ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π",
                "–ö—ç—à–±—ç–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã",
                "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ª–∏–º–∏—Ç–æ–≤"
            ]
        elif "–ó—Ä–µ–ª—ã–µ" in lifecycle_stage:
            actions = [
                "–ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏",
                "–ü—Ä–µ–º–∏—É–º –ø—Ä–æ–¥—É–∫—Ç—ã",
                "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
                "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"
            ]
        elif "–ß–µ–º–ø–∏–æ–Ω—ã" in lifecycle_stage:
            actions = [
                "VIP —Å—Ç–∞—Ç—É—Å",
                "–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
                "–†–µ—Ñ–µ—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã",
                "–ö–æ–Ω—Å—å–µ—Ä–∂ —Å–µ—Ä–≤–∏—Å—ã"
            ]
        else:
            actions = [
                "–ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                "–ö—Ä–æ—Å—Å-–ø—Ä–æ–¥–∞–∂–∏",
                "–°–µ–∑–æ–Ω–Ω—ã–µ –∞–∫—Ü–∏–∏",
                "–£–ª—É—á—à–µ–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–∞"
            ]
        
        return actions
    
    def create_monitoring_dashboard_data(self) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        print("\nüìä –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê")
        print("="*50)
        
        monitoring_data = {
            'timestamp': pd.Timestamp.now(),
            'segments_overview': {},
            'alerts': [],
            'kpi_trends': {},
            'recommendations': {}
        }
        
        
        for cluster_id in sorted(self.cluster_df['cluster'].unique()):
            cluster_data = self.cluster_df[self.cluster_df['cluster'] == cluster_id]
            
            segment_kpis = {
                'size': len(cluster_data),
                'avg_transactions': cluster_data['total_transactions'].mean(),
                'avg_revenue': cluster_data['total_amount'].mean(),
                'avg_frequency': cluster_data['total_transactions'].mean() / 12,  
                'retention_proxy': cluster_data['spending_consistency'].mean(),
                'growth_potential': self._calculate_growth_potential(cluster_data)
            }
            
            monitoring_data['segments_overview'][cluster_id] = segment_kpis
            
            
            alerts = self._generate_segment_alerts(cluster_id, segment_kpis, cluster_data)
            monitoring_data['alerts'].extend(alerts)
        
        
        monitoring_path = config.get_output_path('monitoring_data.json')
        with open(monitoring_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_for_json(monitoring_data), f, ensure_ascii=False, indent=2, default=str)
        
        print(f"üìä –î–∞–Ω–Ω—ã–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {monitoring_path}")
        print(f"üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–ª–µ—Ä—Ç–æ–≤: {len(monitoring_data['alerts'])}")
        
        return monitoring_data
    
    def _calculate_growth_potential(self, cluster_data: pd.DataFrame) -> str:
        """–†–∞—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ä–æ—Å—Ç–∞ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        avg_transactions = cluster_data['total_transactions'].mean()
        avg_merchants = cluster_data['unique_merchants'].mean()
        spending_consistency = cluster_data['spending_consistency'].mean()
        
        
        transaction_score = min(1.0, avg_transactions / 100)
        merchant_score = min(1.0, avg_merchants / 50)
        consistency_score = spending_consistency
        
        growth_score = (transaction_score + merchant_score + consistency_score) / 3
        
        if growth_score >= 0.8:
            return "–ù–∏–∑–∫–∏–π (—É–∂–µ —Ä–∞–∑–≤–∏—Ç—ã–π)"
        elif growth_score >= 0.6:
            return "–°—Ä–µ–¥–Ω–∏–π"
        elif growth_score >= 0.4:
            return "–í—ã—Å–æ–∫–∏–π"
        else:
            return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π"
    
    def _generate_segment_alerts(self, cluster_id: int, kpis: Dict[str, float], cluster_data: pd.DataFrame) -> List[Dict[str, str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
        alerts = []
        
        
        if kpis['avg_transactions'] < 5:
            alerts.append({
                'segment_id': cluster_id,
                'type': 'LOW_ACTIVITY',
                'severity': 'HIGH',
                'message': f'–°–µ–≥–º–µ–Ω—Ç {cluster_id}: –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å ({kpis["avg_transactions"]:.1f} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π)',
                'recommendation': '–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ä–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—É—é –∫–∞–º–ø–∞–Ω–∏—é'
            })
        
        
        if kpis['retention_proxy'] < 0.3:
            alerts.append({
                'segment_id': cluster_id,
                'type': 'CHURN_RISK',
                'severity': 'HIGH',
                'message': f'–°–µ–≥–º–µ–Ω—Ç {cluster_id}: –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞ (–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å {kpis["retention_proxy"]:.2f})',
                'recommendation': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —É–¥–µ—Ä–∂–∞–Ω–∏—è'
            })
        
        
        if kpis['size'] > len(self.cluster_df) * 0.3 and kpis['avg_revenue'] < self.cluster_df['total_amount'].median():
            alerts.append({
                'segment_id': cluster_id,
                'type': 'LARGE_LOW_VALUE',
                'severity': 'MEDIUM',
                'message': f'–°–µ–≥–º–µ–Ω—Ç {cluster_id}: –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä ({kpis["size"]} –∫–ª–∏–µ–Ω—Ç–æ–≤) –Ω–æ –Ω–∏–∑–∫–∏–π –¥–æ—Ö–æ–¥',
                'recommendation': '–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞'
            })
        
        
        if kpis['growth_potential'] == "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π":
            alerts.append({
                'segment_id': cluster_id,
                'type': 'GROWTH_OPPORTUNITY',
                'severity': 'LOW',
                'message': f'–°–µ–≥–º–µ–Ω—Ç {cluster_id}: –í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞',
                'recommendation': '–ò–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞'
            })
        
        return alerts
    
    def design_ab_test_framework(self) -> Dict[str, Any]:
        """–î–∏–∑–∞–π–Ω A/B —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
        print("\nüß™ –î–ò–ó–ê–ô–ù A/B –¢–ï–°–¢–û–í")
        print("="*50)
        
        ab_tests = {}
        
        for cluster_id in sorted(self.cluster_df['cluster'].unique()):
            cluster_data = self.cluster_df[self.cluster_df['cluster'] == cluster_id]
            segment_type = self.segment_profiles[cluster_id]['segment_type']['name']
            
            
            if 'üò¥ –°–ø—è—â–∏–µ' in segment_type:
                test_scenarios = self._design_reactivation_tests(cluster_data)
            elif '‚ö° –ê–∫—Ç–∏–≤–Ω—ã–µ' in segment_type:
                test_scenarios = self._design_engagement_tests(cluster_data)
            elif 'üåü –ü—Ä–µ–º–∏—É–º' in segment_type or 'üíé VIP' in segment_type:
                test_scenarios = self._design_premium_tests(cluster_data)
            else:
                test_scenarios = self._design_general_tests(cluster_data)
            
            ab_tests[cluster_id] = {
                'segment_type': segment_type,
                'segment_size': len(cluster_data),
                'test_scenarios': test_scenarios,
                'sample_size_recommendation': self._calculate_sample_size(len(cluster_data)),
                'test_duration_weeks': self._recommend_test_duration(cluster_data),
                'success_metrics': self._define_success_metrics(segment_type)
            }
            
            print(f"\nüéØ –°–ï–ì–ú–ï–ù–¢ {cluster_id} ({segment_type}):")
            print(f"   –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {ab_tests[cluster_id]['sample_size_recommendation']}")
            print(f"   –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–∞: {ab_tests[cluster_id]['test_duration_weeks']} –Ω–µ–¥–µ–ª—å")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤: {len(test_scenarios)}")
        
        
        ab_test_path = config.get_output_path('ab_test_plan.json')
        with open(ab_test_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_for_json(ab_tests), f, ensure_ascii=False, indent=2)
        
        print(f"\nüìã –ü–ª–∞–Ω A/B —Ç–µ—Å—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {ab_test_path}")
        
        return ab_tests
    
    def _design_reactivation_tests(self, cluster_data: pd.DataFrame) -> List[Dict[str, str]]:
        """–î–∏–∑–∞–π–Ω —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —Ä–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–ø—è—â–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return [
            {
                'test_name': '–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è vs –û–±—â–∏–µ –∞–∫—Ü–∏–∏',
                'hypothesis': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É–≤–µ–ª–∏—á–∞—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ 25%',
                'control_group': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ email-—Ä–∞—Å—Å—ã–ª–∫–∏',
                'test_group': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏',
                'primary_metric': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ –º–µ—Å—è—Ü',
                'secondary_metrics': ['–°—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –¥–Ω–µ–π']
            },
            {
                'test_name': '–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç vs –°–∫–∏–¥–∫–∏',
                'hypothesis': '–û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–≤—ã—Å–∏—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å',
                'control_group': '–°–∫–∏–¥–∫–∏ –∏ –ø—Ä–æ–º–æ–∫–æ–¥—ã',
                'test_group': '–û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã + –Ω–µ–±–æ–ª—å—à–∏–µ –±–æ–Ω—É—Å—ã',
                'primary_metric': 'Retention rate —á–µ—Ä–µ–∑ 3 –º–µ—Å—è—Ü–∞',
                'secondary_metrics': ['Engagement —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º', 'NPS']
            }
        ]
    
    def _design_engagement_tests(self, cluster_data: pd.DataFrame) -> List[Dict[str, str]]:
        """–î–∏–∑–∞–π–Ω —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return [
            {
                'test_name': '–ì–µ–π–º–∏—Ñ–∏–∫–∞—Ü–∏—è vs –ö—ç—à–±—ç–∫',
                'hypothesis': '–ò–≥—Ä–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —É–≤–µ–ª–∏—á–∞—Ç —á–∞—Å—Ç–æ—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞ 15%',
                'control_group': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫—ç—à–±—ç–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∞',
                'test_group': '–°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –∏ —É—Ä–æ–≤–Ω–µ–π',
                'primary_metric': '–ß–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
                'secondary_metrics': ['–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ä—á–∞–Ω—Ç–æ–≤', '–í—Ä–µ–º—è –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏']
            },
            {
                'test_name': '–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ vs –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã',
                'hypothesis': '–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ–≤—ã—Å—è—Ç –ª–æ—è–ª—å–Ω–æ—Å—Ç—å',
                'control_group': '–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã',
                'test_group': '–†–µ—Ñ–µ—Ä–∞–ª—å–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ + —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —á–µ–ª–ª–µ–Ω–¥–∂–∏',
                'primary_metric': 'Customer Lifetime Value',
                'secondary_metrics': ['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Ñ–µ—Ä–∞–ª–æ–≤', '–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å']
            }
        ]
    
    def _design_premium_tests(self, cluster_data: pd.DataFrame) -> List[Dict[str, str]]:
        """–î–∏–∑–∞–π–Ω —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–º–∏—É–º –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        return [
            {
                'test_name': '–ö–æ–Ω—Å—å–µ—Ä–∂-—Å–µ—Ä–≤–∏—Å vs –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞',
                'hypothesis': '–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å —É–≤–µ–ª–∏—á–∏—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ 20%',
                'control_group': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞',
                'test_group': '–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä',
                'primary_metric': 'Net Promoter Score (NPS)',
                'secondary_metrics': ['Customer Satisfaction', '–í—Ä–µ–º—è —Ä–µ—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤']
            },
            {
                'test_name': '–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã vs –£–ª—É—á—à–µ–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è',
                'hypothesis': '–≠–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å –ø–æ–≤—ã—Å–∏—Ç –ª–æ—è–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–µ —á–µ–º –ª—å–≥–æ—Ç—ã',
                'control_group': '–£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ç–∞—Ä–∏—Ñ—ã',
                'test_group': '–î–æ—Å—Ç—É–ø –∫ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º',
                'primary_metric': 'Retention rate',
                'secondary_metrics': ['Cross-sell success', 'Wallet share']
            }
        ]
    
    def _design_general_tests(self, cluster_data: pd.DataFrame) -> List[Dict[str, str]]:
        """–î–∏–∑–∞–π–Ω –æ–±—â–∏—Ö —Ç–µ—Å—Ç–æ–≤"""
        return [
            {
                'test_name': '–ú–æ–±–∏–ª—å–Ω—ã–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è vs Email',
                'hypothesis': 'Push-—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ email –¥–ª—è –≤–æ–≤–ª–µ—á–µ–Ω–∏—è',
                'control_group': 'Email-—Ä–∞—Å—Å—ã–ª–∫–∏',
                'test_group': 'Push-—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è',
                'primary_metric': 'Click-through rate',
                'secondary_metrics': ['Conversion rate', 'App engagement']
            },
            {
                'test_name': '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–∫—Ü–∏–∏ vs –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã',
                'hypothesis': '–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∞–∫—Ü–∏–∏ —Å–æ–∑–¥–∞—é—Ç –±–æ–ª—å—à–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏',
                'control_group': '–ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏',
                'test_group': '–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–∫—Ü–∏–∏',
                'primary_metric': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
                'secondary_metrics': ['–°—Ä–µ–¥–Ω–∏–π —á–µ–∫', '–ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è']
            }
        ]
    
    def _calculate_sample_size(self, segment_size: int) -> Dict[str, int]:
        """–†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è A/B —Ç–µ—Å—Ç–∞"""
        
        min_sample = max(100, int(segment_size * 0.1))  
        recommended_sample = max(500, int(segment_size * 0.2))  
        max_sample = min(segment_size // 2, int(segment_size * 0.5))  
        
        return {
            'minimum': min_sample,
            'recommended': recommended_sample,
            'maximum': max_sample
        }
    
    def _recommend_test_duration(self, cluster_data: pd.DataFrame) -> int:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–∞ –≤ –Ω–µ–¥–µ–ª—è—Ö"""
        avg_frequency = cluster_data['total_transactions'].mean() / 12  
        
        if avg_frequency >= 10:  
            return 2  
        elif avg_frequency >= 5:  
            return 4  
        elif avg_frequency >= 2:  
            return 6  
        else:  
            return 8  
    
    def _define_success_metrics(self, segment_type: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —É—Å–ø–µ—Ö–∞ –¥–ª—è —Ç–∏–ø–∞ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        base_metrics = ['–ö–æ–Ω–≤–µ—Ä—Å–∏—è', '–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å']
        
        if 'üò¥ –°–ø—è—â–∏–µ' in segment_type:
            return base_metrics + ['–†–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è', '–í—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏']
        elif '‚ö° –ê–∫—Ç–∏–≤–Ω—ã–µ' in segment_type:
            return base_metrics + ['–ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è', '–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏']
        elif 'üåü –ü—Ä–µ–º–∏—É–º' in segment_type or 'üíé VIP' in segment_type:
            return base_metrics + ['NPS', 'Wallet share', 'Retention']
        else:
            return base_metrics + ['Cross-sell', '–°—Ä–µ–¥–Ω–∏–π —á–µ–∫'] 
    
    def generate_executive_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\nüìã –°–û–ó–î–ê–ù–ò–ï –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ì–û –û–¢–ß–ï–¢–ê")
        print("="*50)
        
        report_lines = []
        
        
        report_lines.extend([
            "# –û–¢–ß–ï–¢ –ü–û –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò –ë–ê–ù–ö–û–í–°–ö–ò–• –ö–õ–ò–ï–ù–¢–û–í",
            f"**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {pd.Timestamp.now().strftime('%d.%m.%Y %H:%M')}",
            f"**–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤:** {len(self.cluster_df):,}",
            f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤:** {len(self.segment_profiles)}",
            "",
            "## üìä –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï",
            ""
        ])
        
        
        segments_by_size = sorted(self.segment_profiles.items(), 
                                key=lambda x: x[1]['percentage'], reverse=True)
        
        for cluster_id, profile in segments_by_size:
            segment_type = profile['segment_type']
            characteristics = profile['characteristics']
            
            report_lines.extend([
                f"### {segment_type['name']} (–°–µ–≥–º–µ–Ω—Ç {cluster_id})",
                f"- **–†–∞–∑–º–µ—Ä:** {profile['size']:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({profile['percentage']:.1f}%)",
                f"- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** {segment_type['priority']}",
                f"- **–°—Ä–µ–¥–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏:** {characteristics['avg_transactions']:.1f}",
                f"- **–°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥:** {characteristics['avg_total_amount']:,.0f} —Ç–µ–Ω–≥–µ",
                f"- **–°—Ä–µ–¥–Ω–∏–π —á–µ–∫:** {characteristics['avg_amount']:,.0f} —Ç–µ–Ω–≥–µ",
                ""
            ])
        
        
        clv_metrics = self.calculate_customer_lifetime_value()
        report_lines.extend([
            "## üí∞ –ê–ù–ê–õ–ò–ó –¶–ï–ù–ù–û–°–¢–ò –ö–õ–ò–ï–ù–¢–û–í (CLV)",
            ""
        ])
        
        for cluster_id in sorted(clv_metrics.keys()):
            clv_data = clv_metrics[cluster_id]
            segment_name = self.segment_profiles[cluster_id]['segment_type']['name']
            
            report_lines.extend([
                f"### {segment_name}",
                f"- **CLV (—Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π):** {clv_data['adjusted_clv']:,.0f} —Ç–µ–Ω–≥–µ",
                f"- **–ú–µ—Å—è—á–Ω—ã–π –¥–æ—Ö–æ–¥:** {clv_data['avg_monthly_revenue']:,.0f} —Ç–µ–Ω–≥–µ",
                f"- **–í—Ä–µ–º—è –∂–∏–∑–Ω–∏:** {clv_data['estimated_lifetime_months']:.1f} –º–µ—Å—è—Ü–µ–≤",
                f"- **–£—Ä–æ–≤–µ–Ω—å —Ü–µ–Ω–Ω–æ—Å—Ç–∏:** {clv_data['value_tier']}",
                ""
            ])
        
        
        report_lines.extend([
            "## üéØ –ö–õ–Æ–ß–ï–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò",
            ""
        ])
        
        
        high_priority_segments = [s for s in segments_by_size 
                                if s[1]['segment_type']['priority'] == '–í—ã—Å–æ–∫–∏–π']
        
        if high_priority_segments:
            report_lines.extend([
                "### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:",
                ""
            ])
            
            for i, (cluster_id, profile) in enumerate(high_priority_segments[:3], 1):
                segment_name = profile['segment_type']['name']
                if cluster_id in self.business_recommendations:
                    strategies = self.business_recommendations[cluster_id]['strategies'][:2]
                    report_lines.append(f"{i}. **{segment_name}:**")
                    for strategy in strategies:
                        report_lines.append(f"   - {strategy}")
                    report_lines.append("")
        
        
        monitoring_data = self.create_monitoring_dashboard_data()
        high_severity_alerts = [alert for alert in monitoring_data['alerts'] 
                              if alert['severity'] == 'HIGH']
        
        if high_severity_alerts:
            report_lines.extend([
                "## üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ê–õ–ï–†–¢–´",
                ""
            ])
            
            for alert in high_severity_alerts:
                report_lines.extend([
                    f"- **{alert['message']}**",
                    f"  *–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:* {alert['recommendation']}",
                    ""
                ])
        
        
        growth_opportunities = []
        for cluster_id, profile in self.segment_profiles.items():
            if cluster_id in clv_metrics:
                clv_data = clv_metrics[cluster_id]
                if "–í—ã—Å–æ–∫–∏–π" in monitoring_data['segments_overview'][cluster_id]['growth_potential']:
                    growth_opportunities.append((cluster_id, profile, clv_data))
        
        if growth_opportunities:
            report_lines.extend([
                "## üìà –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –†–û–°–¢–ê",
                ""
            ])
            
            for cluster_id, profile, clv_data in growth_opportunities:
                segment_name = profile['segment_type']['name']
                potential_revenue = clv_data['adjusted_clv'] * profile['size']
                
                report_lines.extend([
                    f"### {segment_name}",
                    f"- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥:** {potential_revenue:,.0f} —Ç–µ–Ω–≥–µ",
                    f"- **–†–∞–∑–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞:** {profile['size']:,} –∫–ª–∏–µ–Ω—Ç–æ–≤",
                    f"- **–¢–µ–∫—É—â–∏–π CLV:** {clv_data['adjusted_clv']:,.0f} —Ç–µ–Ω–≥–µ",
                    ""
                ])
        
        
        total_clv = sum(clv_metrics[cid]['adjusted_clv'] * self.segment_profiles[cid]['size'] 
                       for cid in clv_metrics.keys())
        
        report_lines.extend([
            "## üìã –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï",
            "",
            f"- **–û–±—â–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π –±–∞–∑—ã:** {total_clv:,.0f} —Ç–µ–Ω–≥–µ",
            f"- **–°—Ä–µ–¥–Ω–∏–π CLV:** {total_clv / len(self.cluster_df):,.0f} —Ç–µ–Ω–≥–µ –Ω–∞ –∫–ª–∏–µ–Ω—Ç–∞",
            f"- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤:** {len(high_severity_alerts)}",
            f"- **–°–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º —Ä–æ—Å—Ç–∞:** {len(growth_opportunities)}",
            "",
            "---",
            "*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∏—Å—Ç–µ–º–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤*"
        ])
        
        
        report_content = "\n".join(report_lines)
        report_path = config.get_output_path('executive_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"üìã –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
        return report_content
    
    def create_presentation_slides(self) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏"""
        print("\nüé® –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ü–†–ï–ó–ï–ù–¢–ê–¶–ò–ò")
        print("="*50)
        
        slides_data = {
            'title_slide': {
                'title': '–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤',
                'subtitle': f'–ê–Ω–∞–ª–∏–∑ {len(self.cluster_df):,} –∫–ª–∏–µ–Ω—Ç–æ–≤',
                'date': pd.Timestamp.now().strftime('%d.%m.%Y'),
                'segments_count': len(self.segment_profiles)
            },
            'overview_slide': {
                'total_clients': len(self.cluster_df),
                'segments': []
            },
            'clv_slide': {
                'title': 'Customer Lifetime Value –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º',
                'data': []
            },
            'recommendations_slide': {
                'title': '–ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏',
                'high_priority': [],
                'medium_priority': [],
                'low_priority': []
            },
            'alerts_slide': {
                'title': '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã',
                'alerts': []
            }
        }
        
        
        segments_by_size = sorted(self.segment_profiles.items(), 
                                key=lambda x: x[1]['percentage'], reverse=True)
        
        for cluster_id, profile in segments_by_size:
            slides_data['overview_slide']['segments'].append({
                'name': profile['segment_type']['name'],
                'size': profile['size'],
                'percentage': profile['percentage'],
                'priority': profile['segment_type']['priority'],
                'avg_transactions': profile['characteristics']['avg_transactions'],
                'avg_revenue': profile['characteristics']['avg_total_amount']
            })
        
        
        clv_metrics = self.calculate_customer_lifetime_value()
        for cluster_id in sorted(clv_metrics.keys()):
            clv_data = clv_metrics[cluster_id]
            segment_name = self.segment_profiles[cluster_id]['segment_type']['name']
            
            slides_data['clv_slide']['data'].append({
                'segment_name': segment_name,
                'clv': clv_data['adjusted_clv'],
                'monthly_revenue': clv_data['avg_monthly_revenue'],
                'lifetime_months': clv_data['estimated_lifetime_months'],
                'value_tier': clv_data['value_tier']
            })
        
        
        for cluster_id, recommendations in self.business_recommendations.items():
            segment_name = self.segment_profiles[cluster_id]['segment_type']['name']
            priority = recommendations['priority']
            
            rec_data = {
                'segment_name': segment_name,
                'strategies': recommendations['strategies'][:3],  # –¢–æ–ø-3
                'expected_impact': recommendations['expected_impact']
            }
            
            if priority == '–í—ã—Å–æ–∫–∏–π':
                slides_data['recommendations_slide']['high_priority'].append(rec_data)
            elif priority == '–°—Ä–µ–¥–Ω–∏–π':
                slides_data['recommendations_slide']['medium_priority'].append(rec_data)
            else:
                slides_data['recommendations_slide']['low_priority'].append(rec_data)
        
        
        monitoring_data = self.create_monitoring_dashboard_data()
        for alert in monitoring_data['alerts']:
            if alert['severity'] in ['HIGH', 'MEDIUM']:
                slides_data['alerts_slide']['alerts'].append({
                    'message': alert['message'],
                    'severity': alert['severity'],
                    'recommendation': alert['recommendation']
                })
        
        
        presentation_path = config.get_output_path('presentation_data.json')
        with open(presentation_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_for_json(slides_data), f, ensure_ascii=False, indent=2)
        
        print(f"üé® –î–∞–Ω–Ω—ã–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {presentation_path}")
        
        return slides_data